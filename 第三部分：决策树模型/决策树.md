# 决策树

## 基本流程与特性

决策树是一类常见的机器学习算法。

一般的，决策树包含一个根节点、若干个内部节点和若干个叶结点。叶结点对应于决策结果，其他的每个结点则对应一个属性测试。

决策树的生成是一个递归过程，有三种情形会导致递归返回：

1. 当前结点包含的样本全属于一个类别；
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；
3. 当前结点包含的样本集合为空，无法划分。

## 划分选择

决策树如何选择最优划分属性是非常重要的，我们当然希望决策树的分支结点所包含的样本尽可能属于同一个类别，即结点的纯度越来越高，于是我们引入一个信息熵的概念。

**信息熵**是度量样本集合纯度最常用的一种指标。假定当前样本集合$D$中第$k$类样本所占的比例为$p_k$，则$D$的信息熵定义为：
$$
Ent(D)=-\Sigma_{k=1}^{|\gamma|}p_klog_2p_k
$$
并且有：$Ent(D)$的值越小，$D$的纯度越高。

**信息增益**是判断某个属性进行划分时，结点纯度提升多少的标准。一般而言，信息增益越大，意味着使用这个属性来进行划分所获得的纯度提升越大。著名的$ID3$决策树学习算法就是以信息增益为准则来选择划分属性。我们假定离散属性$a$有$V$个可能的取值，若使用$a$来对集合$D$进行划分，则会产生$V$个分支结点，其中第$v$个分支包含了$D$在所有属性$a$上取值为$a^v$的样本，记为$D^v$，则属性$a$对样本集$D$进行划分所获得的的信息增益为：
$$
Gain(D,a)=Ent(D)-\Sigma_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
$$
然而，并不是所有的决策树都使用增益率作为最优划分属性的标准：

$C4.5$决策树算法使用：
$$
Gain\_radio(D,a)=\frac{Gain(D,a)}{IV(a)}\newline
IV(a)=-\Sigma_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}
$$
$CART$决策树使用基尼系数：
$$
Gini(D)=1-\Sigma_{k=1}^{|\gamma|}p_k^2\newline
Gini\_index(D,a)=\Sigma_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)\newline
a_*=argmin_{a \in A}Gini\_index(D,a)
$$

## 剪枝处理

剪枝是决策树算法对付过拟合的主要手段，主要分为以下两种：

1. 预剪枝：划分前进行估计，判断是否能对决策树带来泛化性能提升；
2. 后剪枝：训练集生成一棵完整的决策树，然后自底向上的对非叶子结点进行考察。

## 决策树优缺点

### 优点

1. 直观，好理解，便于可视化；
1. 在决策树的训练过程中，可以根据最优划分属性的标准，得到各个属性的权重；
2. 可以处理含有缺失值的数据。

### 缺点

1. 对于树的最大深度这个参数非常敏感；
2. 对于类别越多的数据，判断的正确性越低；
3. 计算效率不高。
