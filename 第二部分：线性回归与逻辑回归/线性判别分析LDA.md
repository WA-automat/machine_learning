# 线性判别分析LDA

## 基本思想

$LDA$的思想非常朴素，给定训练样例集，设法将样例投影到一条直线上，使得同类样例的**投影点**尽可能接近、异类样例点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。

## 损失函数

使得同类投影点协方差尽可能小，类中心之间的距离尽可能大，即最大化$J$：
$$
J=\frac{||\omega^T\mu_0-\omega^T\mu_1||_2^2}{\omega^T\Sigma_0\omega+\omega^T\Sigma_1\omega}\\
=\frac{\omega^T(\mu_0-\mu_1)(\mu_0-\mu_1)^T\omega}{\omega^T(\Sigma_0+\Sigma_1)\omega}
$$
其中，分母是同类投影点的协方差，分子是类中心之间的距离。

定义类内散度矩阵$S_w=\Sigma_0+\Sigma_1$与类间散度矩阵$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$
$$
J=\frac{\omega^TS_b\omega}{\omega^TS_w\omega}
$$

$LDA$算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。在我们进行图像识别图像识别相关的数据分析时，LDA是一个有力的工具。下面总结下$LDA$算法的优缺点。

$LDA$算法的主要优点有：

1. 在降维过程中可以使用类别的先验知识经验，而像$PCA$这样的无监督学习则无法使用类别先验知识。

2. $LDA$在样本分类信息依赖均值而不是方差的时候，比$PCA$之类的算法较优。

$LDA$算法的主要缺点有：

1. $LDA$不适合对非高斯分布样本进行降维，$PCA$也有这个问题。

2. $LDA$降维最多降到类别数$k-1$的维数，如果我们降维的维度大于$k-1$，则不能使用$LDA$。当然目前有一些$LDA$的进化版算法可以绕过这个问题。

3. $LDA$在样本分类信息依赖方差而不是均值的时候，降维效果不好。

4. $LDA$可能过度拟合数据。
